{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_autoencoder_dots.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LzOi2yfpMe1PLkYry1kJ-DalsYMvKwCV",
      "authorship_tag": "ABX9TyPmfS5tKQEqvleWIWlnyPRk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niz11/Bachelor_Thesis_Colab_Files/blob/main/train_autoencoder_dots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC3DKsS_2bkK"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "data = np.load('drive/My Drive/moving_dots.npy')\n",
        "\n",
        "trainng_size = 98000\n",
        "test_size = 2000\n",
        "train_loop = 4900\n",
        "test_loop = 100\n",
        "\n",
        "\n",
        "input_train = np.empty((trainng_size,64,64,1),  dtype=np.uint8)\n",
        "input_test = np.empty((test_size,64,64,1),  dtype=np.uint8)\n",
        "\n",
        "index = 0\n",
        "for i in range(train_loop):\n",
        "    for j in range(20):\n",
        "        input_train[index] = data[i][j]\n",
        "        index +=1\n",
        "\n",
        "index = 0            \n",
        "for i in range(test_loop):\n",
        "    for j in range(10):\n",
        "        input_test[index] = data[i][j]\n",
        "        index +=1\n",
        "\n",
        "print(data.shape)\n",
        "print(input_train.shape)\n",
        "print(input_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE6QSX7Y2dR9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Test By visualisation \n",
        "n = 20\n",
        "randomSmaple = np.random.randint(750, size=1)\n",
        "x = randomSmaple * 20\n",
        "print(x)\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    if i < 10:\n",
        "      plt.imshow(input_train[i + x].reshape(64, 64))\n",
        "    else:\n",
        "      plt.imshow(input_train[i + x].reshape(64, 64))\n",
        "\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xp-AOSN2frz"
      },
      "source": [
        "# Data & model configuration\n",
        "img_width, img_height = 64, 64\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "latent_dim = 64\n",
        "num_channels = 1\n",
        "\n",
        "input_shape = (img_height, img_width, num_channels)\n",
        "\n",
        "# Normalize data\n",
        "input_train = input_train / 255\n",
        "input_test = input_test / 255\n",
        "\n",
        "# # =================\n",
        "# # Encoder\n",
        "# # =================\n",
        "\n",
        "# Definition\n",
        "inputs       = Input(shape=input_shape, name='encoder_input')\n",
        "encodingLayers  = Conv2D(filters=128, kernel_size=5, strides=1, padding='same', activation='relu')(inputs)\n",
        "encodingLayers  = BatchNormalization()(encodingLayers)\n",
        "encodingLayers  = Conv2D(filters=256, kernel_size=5, strides=2, padding='same', activation='relu')(encodingLayers)\n",
        "encodingLayers  = BatchNormalization()(encodingLayers)\n",
        "encodingLayers  = Conv2D(filters=512, kernel_size=5, strides=1, padding='same', activation='relu')(encodingLayers)\n",
        "encodingLayers  = BatchNormalization()(encodingLayers)\n",
        "encodingLayers  = Conv2D(filters=1024, kernel_size=5, strides=2, padding='same', activation='relu')(encodingLayers)\n",
        "encodingLayers  = BatchNormalization()(encodingLayers)\n",
        "output       = Flatten()(encodingLayers)\n",
        "output       = Dense(latent_dim, activation='relu')(output)\n",
        "output       = BatchNormalization()(output)\n",
        "\n",
        "# Get Conv2D shape for Conv2DTranspose operation in decoder\n",
        "conv_shape = K.int_shape(encodingLayers)\n",
        "\n",
        "# Instantiate encoder\n",
        "encoder = Model(inputs, output, name='encoder')\n",
        "encoder.summary()\n",
        "\n",
        "# =================\n",
        "# Decoder\n",
        "# =================\n",
        "\n",
        "# Definition\n",
        "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
        "decoder_inputs  = Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(decoder_input)\n",
        "decoder_inputs  = BatchNormalization()(decoder_inputs)\n",
        "decoder_inputs  = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(decoder_inputs)\n",
        "decoder_laters = Conv2DTranspose(filters=1024, kernel_size=5, strides=1, padding='same', activation='relu')(decoder_inputs)\n",
        "decoder_laters = BatchNormalization()(decoder_laters)\n",
        "decoder_laters = Conv2DTranspose(filters=512, kernel_size=5, strides=2, padding='same', activation='relu')(decoder_laters)\n",
        "decoder_laters = BatchNormalization()(decoder_laters)\n",
        "decoder_laters = Conv2DTranspose(filters=256, kernel_size=5, strides=1, padding='same', activation='relu')(decoder_laters)\n",
        "decoder_laters = BatchNormalization()(decoder_laters)\n",
        "decoder_laters = Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same', activation='relu')(decoder_laters)\n",
        "decoder_laters = BatchNormalization()(decoder_laters)\n",
        "decoder_output     = Conv2DTranspose(filters=num_channels, kernel_size=3, activation='sigmoid', padding='same', name='decoder_output')(decoder_laters)\n",
        "\n",
        "# Instantiate decoder\n",
        "decoder = Model(decoder_input, decoder_output, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "# Instantiate VAE\n",
        "model_outputs = decoder(encoder(inputs))\n",
        "model = Model(inputs, model_outputs, name='model')\n",
        "model.summary()\n",
        "\n",
        "# checkpoint\n",
        "checkpoint_path = \"drive/My Drive/thesis/dot_encoder/save/cp-{epoch:04d}.ckpt\"\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 period=1,\n",
        "                                                 verbose=1)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.fit(input_train, input_train, epochs = epochs, batch_size = batch_size, validation_split = 0.2, callbacks=[cp_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCI3GhOm2icj"
      },
      "source": [
        "encdoing = encoder.predict(input_test)\n",
        "print(encdoing.shape)\n",
        "decoded = decoder.predict(encdoing)\n",
        "print(decoded.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP-gPw2K2lPG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for j in range(10,15):\n",
        "  n = 20\n",
        "  # randomSmaple = np.random.randint(4, size=1)\n",
        "  x = j * 20\n",
        "\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  for i in range(n):\n",
        "      # display original\n",
        "      ax = plt.subplot(2, n, i + 1)\n",
        "      if i < 10:\n",
        "        plt.imshow(decoded[i+ x].reshape(64, 64))\n",
        "      else:\n",
        "        plt.imshow(decoded[i+ x].reshape(64, 64))\n",
        "\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  for i in range(n):\n",
        "      # display original\n",
        "      ax = plt.subplot(2, n, i + 1)\n",
        "      if i < 10:\n",
        "        plt.imshow(input_test[i+ x].reshape(64, 64))\n",
        "      else:\n",
        "        plt.imshow(input_test[i+ x].reshape(64, 64))\n",
        "\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH9agkGl2nJ8"
      },
      "source": [
        "# serialize model to JSON\n",
        "model_json = encoder.to_json()\n",
        "with open(\"encoderDots2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "encoder.save_weights(\"encoderDots2.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# serialize model to JSON\n",
        "model_json = decoder.to_json()\n",
        "with open(\"decoderDots2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "decoder.save_weights(\"decoderDots2.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}